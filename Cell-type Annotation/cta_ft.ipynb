{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import matplotlib\n",
    "import mygene\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import sklearn\n",
    "import random\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as Fx\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from xgboost import XGBClassifier\n",
    "# import sentence_transformers\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use('seaborn-v0_8-dark-palette')\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Arial\"\n",
    "# })\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "np.random.seed(202310)\n",
    "# use hnswlib for NN classification\n",
    "try:\n",
    "    import hnswlib\n",
    "    hnswlib_imported = True\n",
    "except ImportError:\n",
    "    hnswlib_imported = False\n",
    "    print(\"hnswlib not installed! We highly recommend installing it for fast similarity search.\")\n",
    "    print(\"To install it, run: pip install hnswlib\")\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to control randomness\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train = sc.read(\"/gpfs/gibbs/pi/zhao/wl545/pbmc/datasets/demo_train.h5ad\")\n",
    "adata_test = sc.read(\"/gpfs/gibbs/pi/zhao/wl545/pbmc/datasets/demo_test.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adata_comb = sc.concat([adata_train, adata_test], label = 'combinone', keys = ['train', 'test'])\n",
    "adata_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"/gpfs/gibbs/pi/zhao/tl688/cpsc_finalproject/genept_data/GenePT/ensem_emb_gpt3.5all.pickle\", \"rb\") as fp:\n",
    "    GPT_3_5_gene_embeddings = pickle.load(fp)\n",
    "gene_names= list(adata_comb.var.index)\n",
    "count_missing = 0\n",
    "EMBED_DIM = 1536 # embedding dim from GPT-3.5\n",
    "lookup_embed = np.zeros(shape=(len(gene_names),EMBED_DIM))\n",
    "for i, gene in enumerate(gene_names):\n",
    "    if gene in GPT_3_5_gene_embeddings:\n",
    "        lookup_embed[i,:] = GPT_3_5_gene_embeddings[gene].flatten()\n",
    "    else:\n",
    "        count_missing+=1\n",
    "lookup_embed.shape\n",
    "\n",
    "# lookup_embed = np.random.rand(lookup_embed.shape[0], lookup_embed.shape[1])\n",
    "\n",
    "adata_train =  adata_comb[adata_comb.obs.combinone == 'train']\n",
    "adata_test =  adata_comb[adata_comb.obs.combinone == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder().fit(adata_train.obs.label)\n",
    "\n",
    "train_obs, valid_obs = train_test_split(adata_train.obs_names, test_size=0.1, random_state=1 )\n",
    "adata_train_train = adata_train[train_obs]\n",
    "adata_train_valid = adata_train[valid_obs]\n",
    "\n",
    "train_label = label_encoder.transform(adata_train_train.obs.label)\n",
    "valid_label = label_encoder.transform(adata_train_valid.obs.label)\n",
    "\n",
    "\n",
    "X_train = torch.FloatTensor(adata_train_train.X)\n",
    "\n",
    "train_label = torch.FloatTensor(train_label)\n",
    "\n",
    "batch_size = 512\n",
    "lookup_embed = torch.FloatTensor(lookup_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(lookup_embed.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, len(label_encoder.classes_))\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, inputs):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x) # can have dataset-specific gene embeddings\n",
    "        emb = torch.matmul(inputs, x)\n",
    "        label_out = self.fc3(emb)\n",
    "        return label_out,emb\n",
    "\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "dataset = torch.utils.data.TensorDataset(X_train.to(device), train_label.to(device))\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, data, labels):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs,_ = net(lookup_embed, data)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    return (predicted == labels).sum().item() / len(labels)\n",
    "\n",
    "def model_output(model, data):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    outputs,emb = net(lookup_embed, data)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    return predicted.cpu().numpy(), emb.cpu().detach().numpy()\n",
    "\n",
    "def eval_function(model, adata_train_train, adata_train_valid):\n",
    "    model.eval()\n",
    "    _,genePT_w_emebed_train =  model_output(model, torch.FloatTensor(adata_train_train.X))\n",
    "    _,genePT_w_emebed_test = model_output(model, torch.FloatTensor(adata_train_valid.X))\n",
    "    \n",
    "    y_train = adata_train_train.obs.label\n",
    "    y_test = adata_train_valid.obs.label\n",
    "    \n",
    "    # cell type clustering\n",
    "    # very quick test\n",
    "    k = 10  # number of neighbors\n",
    "    ref_cell_embeddings = genePT_w_emebed_train\n",
    "    test_emebd = genePT_w_emebed_test\n",
    "    neighbors_list_gpt_v2 = []\n",
    "    if hnswlib_imported:\n",
    "        # Declaring index, using most of the default parameters from https://github.com/nmslib/hnswlib\n",
    "        p = hnswlib.Index(space = 'cosine', dim = ref_cell_embeddings.shape[1]) # possible options are l2, cosine or ip\n",
    "        p.init_index(max_elements = ref_cell_embeddings.shape[0], ef_construction = 200, M = 16)\n",
    "\n",
    "        # Element insertion (can be called several times):\n",
    "        p.add_items(ref_cell_embeddings, ids = np.arange(ref_cell_embeddings.shape[0]))\n",
    "\n",
    "        # Controlling the recall by setting ef:\n",
    "        p.set_ef(50) # ef should always be > k\n",
    "\n",
    "        # Query dataset, k - number of closest elements (returns 2 numpy arrays)\n",
    "        labels, distances = p.knn_query(test_emebd, k = k)\n",
    "\n",
    "    idx_list=[i for i in range(test_emebd.shape[0])]\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    for k in idx_list:\n",
    "        # this is the true cell type\n",
    "        gt = y_test[k]\n",
    "        if hnswlib_imported:\n",
    "            idx = labels[k]\n",
    "        else:\n",
    "            idx, sim = get_similar_vectors(test_emebd[k][np.newaxis, ...], ref_cell_embeddings)\n",
    "        pred = mode(y_train[idx], axis=0)\n",
    "        neighbors_list_gpt_v2.append(y_train[idx])\n",
    "        gt_list.append(gt)\n",
    "        pred_list.append(pred[0][0])\n",
    "    acc = sklearn.metrics.accuracy_score(gt_list, pred_list)\n",
    "    return acc\n",
    "\n",
    "from pytorch_metric_learning import miners, losses\n",
    "miner = miners.MultiSimilarityMiner()\n",
    "loss_func = losses.TripletMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prev = 0\n",
    "net.train()\n",
    "model_best = None\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs,emb = net(lookup_embed, inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels) + 100 * loss_func(emb, labels)\n",
    "#         loss = loss_func(emb, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    if epoch % 5 ==0:\n",
    "        eval_acc = eval_function(net, adata_train_train, adata_train_valid)\n",
    "        print(eval_acc)\n",
    "        if eval_acc > prev:\n",
    "            prev = eval_acc\n",
    "            model_best = pickle.loads(pickle.dumps(net))\n",
    "        else:\n",
    "            print(\"stop the training at:\", epoch)\n",
    "            break\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.FloatTensor(adata_test.X)\n",
    "labels = torch.FloatTensor()\n",
    "label_predict,embeddings = model_output(model_best, input_data)\n",
    "outlabel = label_encoder.inverse_transform(label_predict)\n",
    "\n",
    "_,genePT_w_emebed_train =  model_output(model_best, torch.FloatTensor(adata_train.X))\n",
    "_,genePT_w_emebed_test = model_output(model_best, torch.FloatTensor(adata_test.X))\n",
    "\n",
    "y_train = adata_train.obs.label\n",
    "y_test = adata_test.obs.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cell type clustering\n",
    "# very quick test\n",
    "k = 10  # number of neighbors\n",
    "ref_cell_embeddings = genePT_w_emebed_train\n",
    "test_emebd = genePT_w_emebed_test\n",
    "neighbors_list_gpt_v2 = []\n",
    "if hnswlib_imported:\n",
    "    # Declaring index, using most of the default parameters from https://github.com/nmslib/hnswlib\n",
    "    p = hnswlib.Index(space = 'cosine', dim = ref_cell_embeddings.shape[1]) # possible options are l2, cosine or ip\n",
    "    p.init_index(max_elements = ref_cell_embeddings.shape[0], ef_construction = 200, M = 16)\n",
    "\n",
    "    # Element insertion (can be called several times):\n",
    "    p.add_items(ref_cell_embeddings, ids = np.arange(ref_cell_embeddings.shape[0]))\n",
    "\n",
    "    # Controlling the recall by setting ef:\n",
    "    p.set_ef(50) # ef should always be > k\n",
    "\n",
    "    # Query dataset, k - number of closest elements (returns 2 numpy arrays)\n",
    "    labels, distances = p.knn_query(test_emebd, k = k)\n",
    "\n",
    "idx_list=[i for i in range(test_emebd.shape[0])]\n",
    "gt_list = []\n",
    "pred_list = []\n",
    "for k in idx_list:\n",
    "    # this is the true cell type\n",
    "    gt = y_test[k]\n",
    "    if hnswlib_imported:\n",
    "        idx = labels[k]\n",
    "    else:\n",
    "        idx, sim = get_similar_vectors(test_emebd[k][np.newaxis, ...], ref_cell_embeddings)\n",
    "    pred = mode(y_train[idx], axis=0)\n",
    "    neighbors_list_gpt_v2.append(y_train[idx])\n",
    "    gt_list.append(gt)\n",
    "    pred_list.append(pred[0][0])\n",
    "sklearn.metrics.accuracy_score(gt_list, pred_list)\n",
    "\n",
    "print('Precision, Recall, F1 (Marco weighted) for GenePT-w embedding: ', \\\n",
    "      sklearn.metrics.precision_recall_fscore_support(gt_list, pred_list,average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
