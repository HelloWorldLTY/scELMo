{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gears import PertData, GEARS\n",
    "\n",
    "# get data\n",
    "pert_data = PertData('./data')\n",
    "# pert_data = PertData('./data_folder')\n",
    "# load dataset in paper: norman, adamson, dixit.\n",
    "pert_data.load(data_name = 'dixit')\n",
    "# specify data split\n",
    "pert_data.prepare_split(split = 'simulation', seed = 1)\n",
    "# get dataloader with batch size\n",
    "pert_data.get_dataloader(batch_size = 32, test_batch_size = 128)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gears.inference import evaluate, compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gears \n",
    "import pytorch_lightning\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "# import sentence_transformers\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use('seaborn-v0_8-dark-palette')\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "np.random.seed(202310)\n",
    "pytorch_lightning.seed_everything(202310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/gpfs/gibbs/pi/zhao/tl688/GenePT/data_embedding/GPT_3_5_gene_embeddings.pickle\", \"rb\") as fp:\n",
    "    GPT_3_5_gene_embeddings = pickle.load(fp)\n",
    "with open(\"/gpfs/gibbs/pi/zhao/tl688/cpsc_finalproject/genept_data/GenePT/ensem_emb_gpt3.5all.pickle\", \"rb\") as fp:\n",
    "    GPT_3_5_gene_embeddings = pickle.load(fp)\n",
    "gene_names= list(pert_data.adata.var['gene_name'].values)\n",
    "count_missing = 0\n",
    "EMBED_DIM = 1536 # embedding dim from GPT-3.5\n",
    "lookup_embed = np.zeros(shape=(len(gene_names),EMBED_DIM))\n",
    "for i, gene in enumerate(gene_names):\n",
    "    if gene in GPT_3_5_gene_embeddings:\n",
    "        lookup_embed[i,:] = GPT_3_5_gene_embeddings[gene].flatten()\n",
    "    else:\n",
    "        count_missing+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up and train a model\n",
    "gears_model = GEARS(pert_data, device = 'cuda:0', gene_emb = lookup_embed)\n",
    "# gears_model = GEARS(pert_data, device = 'cuda:0')\n",
    "gears_model.model_initialize(hidden_size = 64)\n",
    "\n",
    "gears_model.train(epochs = 20)\n",
    "\n",
    "# # save/load model\n",
    "# gears_model.save_model('gears_dixit_new')\n",
    "# gears_model.load_pretrained('gears_dixit_new')\n",
    "\n",
    "test_res = evaluate(gears_model.dataloader['test_loader'], gears_model.model, gears_model.config['uncertainty'], gears_model.device)\n",
    "test_metrics, test_pert_res = compute_metrics(test_res)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
