{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import matplotlib\n",
    "import mygene\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import sklearn\n",
    "import random\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "# import sentence_transformers\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use('seaborn-v0_8-dark-palette')\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "np.random.seed(202310)\n",
    "# use hnswlib for NN classification\n",
    "try:\n",
    "    import hnswlib\n",
    "    hnswlib_imported = True\n",
    "except ImportError:\n",
    "    hnswlib_imported = False\n",
    "    print(\"hnswlib not installed! We highly recommend installing it for fast similarity search.\")\n",
    "    print(\"To install it, run: pip install hnswlib\")\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "sampled_adata = sc.read(\"/gpfs/gibbs/pi/zhao/tl688/board_heartcell/SCP1303/anndata/Cardiomyocyte_data_subsample0.1.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_per_cell(sampled_adata)\n",
    "sc.pp.log1p(sampled_adata)\n",
    "\n",
    "sampled_adata.uns['log1p']['base'] = None\n",
    "sc.pp.highly_variable_genes(sampled_adata, n_top_genes=2000)\n",
    "sampled_adata = sampled_adata[:,sampled_adata.var.highly_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_comb = sampled_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/gpfs/gibbs/pi/zhao/tl688/cpsc_finalproject/genept_data/GenePT/ensem_emb_gpt3.5all.pickle\", \"rb\") as fp:\n",
    "    GPT_3_5_gene_embeddings = pickle.load(fp)\n",
    "gene_names= list(adata_comb.var.index)\n",
    "count_missing = 0\n",
    "EMBED_DIM = 1536 # embedding dim from GPT-3.5\n",
    "lookup_embed = np.zeros(shape=(len(gene_names),EMBED_DIM))\n",
    "for i, gene in enumerate(gene_names):\n",
    "    if gene in GPT_3_5_gene_embeddings:\n",
    "        lookup_embed[i,:] = GPT_3_5_gene_embeddings[gene].flatten()\n",
    "    else:\n",
    "        count_missing+=1\n",
    "lookup_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obs,test_obs,train_label,test_label = train_test_split(sampled_adata.obs_names, \n",
    "                                                    sampled_adata.obs.disease,\n",
    "                                                    test_size=0.20, random_state=2023)\n",
    "\n",
    "adata_train = sampled_adata[train_obs]\n",
    "adata_test = sampled_adata[test_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lookup_embed = torch.FloatTensor(lookup_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder().fit(adata_train.obs.disease)\n",
    "\n",
    "adata_train_train = adata_train\n",
    "\n",
    "train_label = label_encoder.transform(adata_train_train.obs.disease)\n",
    "\n",
    "\n",
    "X_train = torch.FloatTensor(adata_train_train.X.toarray())\n",
    "\n",
    "train_label = torch.FloatTensor(train_label)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_train, train_label)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(lookup_embed.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, len(label_encoder.classes_))\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, inputs):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        emb = torch.matmul(inputs, x)\n",
    "        label_out = self.fc3(emb)\n",
    "        return label_out,emb\n",
    "\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "dataset = torch.utils.data.TensorDataset(X_train.to(device), train_label.to(device))\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "def model_evaluation(model, data, labels):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs,_ = net(lookup_embed, data)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    return (predicted == labels).sum().item() / len(labels)\n",
    "\n",
    "def model_output(model, data):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    outputs,emb = net(lookup_embed, data)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    return predicted.cpu().numpy(), emb.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import miners, losses\n",
    "miner = miners.MultiSimilarityMiner()\n",
    "loss_func = losses.TripletMarginLoss()\n",
    "\n",
    "prev = 0\n",
    "# here the validing section is not very important, since we pay more attention to generating distinguishable embeddings.\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.long()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs,emb = net(lookup_embed, inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels) + 100 * loss_func(emb, labels)\n",
    "#         loss = loss_func(emb, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,genePT_w_emebed_train =  model_output(net, torch.FloatTensor(adata_train.X.toarray()))\n",
    "_,genePT_w_emebed_test = model_output(net, torch.FloatTensor(adata_test.X.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test.obsm['X_pca'] = genePT_w_emebed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test.obsm['X_genept'] = genePT_w_emebed_test\n",
    "\n",
    "meanv = np.mean(adata_test[adata_test.obs['disease'] == 'NF'].obsm['X_genept'],axis=0)\n",
    "\n",
    "meanv_ascend = np.mean(adata_test[adata_test.obs['disease'] == 'DCM'].obsm['X_genept'],axis=0)\n",
    "\n",
    "import scipy\n",
    "\n",
    "raw_cs = 1 - scipy.spatial.distance.cosine(meanv, meanv_ascend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(202310)\n",
    "sc.tl.rank_genes_groups(sampled_adata, groupby='disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'DCM'\n",
    "control = 'NF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sampled_adata.uns['rank_genes_groups']['names'][disease][0:10]:\n",
    "    adata_test_new = adata_test.copy()\n",
    "    adata_test_new[:,i].X = 0\n",
    "    \n",
    "    _,genePT_w_emebed_test = model_output(net, torch.FloatTensor(adata_test_new.X.toarray()))\n",
    "    adata_test_new.obsm['X_genept'] = genePT_w_emebed_test\n",
    "    meanv = np.mean(adata_test_new[adata_test_new.obs['disease'] == control].obsm['X_genept'],axis=0)\n",
    "    meanv_ascend = np.mean(adata_test_new[adata_test_new.obs['disease'] == disease].obsm['X_genept'],axis=0)\n",
    "#     print(i)\n",
    "    print(1 - scipy.spatial.distance.cosine(meanv, meanv_ascend) - raw_cs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
